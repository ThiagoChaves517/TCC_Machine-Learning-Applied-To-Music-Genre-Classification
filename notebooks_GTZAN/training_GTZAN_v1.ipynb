{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb446644",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d9cf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas RAPIDS (GPU)\n",
    "from cuml.preprocessing import StandardScaler\n",
    "from cuml.pipeline import Pipeline\n",
    "from cuml.neighbors import KNeighborsClassifier\n",
    "from cuml.svm import SVC\n",
    "from cuml.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from cuml.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Bibliotecas Scikit-learn (CPU)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler as SklearnStandardScaler \n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Bibliotecas de Utilidades\n",
    "import cupy as cp \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Bibliotecas TensorFlow\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Configuração de Memória da GPU (Crucial para 6GB VRAM)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6290619",
   "metadata": {},
   "source": [
    "### Audio Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6064b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho para o seu arquivo CSV do GTZAN\n",
    "GTZAN_CSV_PATH = '../gtzan_dataset/features_30_sec.csv'\n",
    "\n",
    "# 1. Carregar Dados\n",
    "df = pd.read_csv(GTZAN_CSV_PATH)\n",
    "\n",
    "# 2. Limpeza\n",
    "# Limpando colunas indesejáveis do GTZAN\n",
    "drop_cols = ['filename', 'length', 'label'] \n",
    "# Verifique se no seu CSV as colunas tem esses nomes exatos, as vezes é 'class' ao invés de 'label'\n",
    "\n",
    "X = df.drop(columns=[c for c in drop_cols if c in df.columns], axis=1)\n",
    "y = df['label']\n",
    "\n",
    "print(f\"Shape dos dados: {X.shape}\")\n",
    "print(f\"Colunas de features: {list(X.columns)}\")\n",
    "\n",
    "# 3. Codificar Labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y).astype(np.int32)\n",
    "\n",
    "# 4. Converter para Numpy e Float32 (Padrão RAPIDS/TF)\n",
    "X_np = X.to_numpy().astype(np.float32)\n",
    "y_np = y_encoded.astype(np.int32)\n",
    "\n",
    "num_classes = len(np.unique(y_np))\n",
    "print(f\"Número de classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0407ab",
   "metadata": {},
   "source": [
    "### Treino dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc10a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gtzan_mlp(input_shape, num_classes):\n",
    "    # Regularização um pouco mais forte pois o dataset é pequeno\n",
    "    l2_reg = 0.02\n",
    "    dropout_rate = 0.6\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_shape,)),\n",
    "        \n",
    "        # Camada 1\n",
    "        layers.Dense(128, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ELU(),\n",
    "        layers.Dropout(dropout_rate),\n",
    "\n",
    "        # Camada 2\n",
    "        layers.Dense(64, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ELU(),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        # Camada 3\n",
    "        layers.Dense(32, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ELU(),\n",
    "        \n",
    "        # Saída\n",
    "        layers.Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform')\n",
    "    ])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005) \n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9777af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Estratégia de Validação\n",
    "# Usamos StratifiedKFold pois não temos Artist ID explícito no CSV padrão\n",
    "n_splits = 10\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# 2. Definição dos Pipelines\n",
    "pipe_knn = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=10)) \n",
    "])\n",
    "\n",
    "pipe_svm = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC(kernel='rbf', C=0.75, gamma='scale')) \n",
    "])\n",
    "\n",
    "# Random Forest: n_estimators mantido, max_depth reduzido levemente\n",
    "pipe_rf = Pipeline([\n",
    "    ('rf', RandomForestClassifier(n_estimators=300, \n",
    "                                  max_depth=7,\n",
    "                                  min_samples_leaf=5,\n",
    "                                  max_features=0.6))\n",
    "])\n",
    "\n",
    "# XGBoost: Ajustado para dataset menor\n",
    "model_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.03,\n",
    "    gamma=5.0,             # AUMENTADO: poda agressiva de árvores\n",
    "    reg_alpha=1.0,\n",
    "    reg_lambda=1.0,        # AUMENTADO: regularização L2\n",
    "    subsample=0.7,         # Usa apenas 70% dos dados por árvore\n",
    "    colsample_bytree=0.7,  # Usa apenas 70% das features por árvore\n",
    "    tree_method='hist',\n",
    "    device=\"cuda\",\n",
    "    random_state=42,\n",
    "    objective='multi:softmax',\n",
    "    num_class=num_classes \n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"KNN (GPU)\": pipe_knn,\n",
    "    \"SVM (GPU)\": pipe_svm,\n",
    "    \"Random Forest (GPU)\": pipe_rf,\n",
    "    \"XGBoost (GPU)\": model_xgb,\n",
    "    \"MLP (Keras)\": \"keras_placeholder\"\n",
    "}\n",
    "\n",
    "cv_scores = {}\n",
    "out_of_fold_preds = {}\n",
    "\n",
    "print(f\"Iniciando treinamento com {n_splits} folds no GTZAN...\")\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nIniciando CV para {model_name}...\")\n",
    "    \n",
    "    fold_scores_acc = []\n",
    "    fold_scores_precision = []\n",
    "    fold_scores_recall = []\n",
    "    fold_scores_f1 = []\n",
    "    \n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    # Não usamos 'groups' aqui\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X_np, y_np)):\n",
    "        X_train, X_test = X_np[train_idx], X_np[test_idx]\n",
    "        y_train, y_test = y_np[train_idx], y_np[test_idx]\n",
    "\n",
    "        # NÃO há Feature Selection (SelectKBest) aqui, usamos todas as features.\n",
    "\n",
    "        y_pred_fold = None \n",
    "\n",
    "        if model_name == \"MLP (Keras)\":\n",
    "            # Scaling na CPU para o TF\n",
    "            scaler = SklearnStandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            keras_model = build_gtzan_mlp(X_train.shape[1], num_classes)\n",
    "            \n",
    "            # Callbacks\n",
    "            callbacks_list = [\n",
    "                EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "            ]\n",
    "\n",
    "            history = keras_model.fit(\n",
    "                X_train_scaled, y_train,\n",
    "                epochs=100, # Mais épocas pois o dataset é pequeno e converge rápido\n",
    "                batch_size=16, # Batch menor para generalizar melhor em dados pequenos\n",
    "                validation_split=0.1,\n",
    "                callbacks=callbacks_list,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            y_probs = keras_model.predict(X_test_scaled, verbose=0)\n",
    "            y_pred_fold = np.argmax(y_probs, axis=1)\n",
    "            \n",
    "            tf.keras.backend.clear_session()\n",
    "            del keras_model\n",
    "            gc.collect()\n",
    "\n",
    "        elif \"(GPU)\" in model_name:\n",
    "            X_train_cp = cp.array(X_train)\n",
    "            X_test_cp = cp.array(X_test)\n",
    "            y_train_cp = cp.array(y_train)\n",
    "            \n",
    "            model.fit(X_train_cp, y_train_cp)\n",
    "            y_pred_cp = model.predict(X_test_cp)\n",
    "            y_pred_fold = cp.asnumpy(y_pred_cp)\n",
    "\n",
    "            # Fazer a predição nos dados de TREINO que acabamos de usar\n",
    "            y_pred_train_cp = model.predict(cp.array(X_train))\n",
    "            y_pred_train = cp.asnumpy(y_pred_train_cp)\n",
    "            # Calcular acurácia de treino\n",
    "            acc_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "            print(f\"Fold {fold} - {model_name} - Treino: {acc_train:.4f}\")\n",
    "\n",
    "            \n",
    "            del X_train_cp, X_test_cp, y_train_cp\n",
    "\n",
    "        # Métricas\n",
    "        acc = accuracy_score(y_test, y_pred_fold)\n",
    "        fold_scores_acc.append(acc)\n",
    "        \n",
    "        # Calculo dos precision_recall_fscore_support\n",
    "        macro_p, macro_r, macro_f1, _ = precision_recall_fscore_support(y_test, y_pred_fold, average='macro', zero_division=0)\n",
    "\n",
    "        # Converter métricas scalarem de volta para CPU/float para armazenar na lista\n",
    "        fold_scores_precision.append(float(macro_p))\n",
    "        fold_scores_recall.append(float(macro_r))\n",
    "        fold_scores_f1.append(float(macro_f1))\n",
    "        \n",
    "        \n",
    "        all_y_true.append(y_test)\n",
    "        all_y_pred.append(y_pred_fold)\n",
    "    \n",
    "    cv_scores[model_name] = {\n",
    "        'Acurácia': np.array(fold_scores_acc),\n",
    "        'Precisão': np.array(fold_scores_precision),\n",
    "        'Recall': np.array(fold_scores_recall),\n",
    "        'F1-Score': np.array(fold_scores_f1)\n",
    "    }\n",
    "    \n",
    "    out_of_fold_preds[model_name] = {\n",
    "        'y_true': np.concatenate(all_y_true),\n",
    "        'y_pred': np.concatenate(all_y_pred)\n",
    "    }\n",
    "    \n",
    "    # Limpeza\n",
    "    gc.collect()\n",
    "    try:\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"\\n--- Avaliação Concluída ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58196d7",
   "metadata": {},
   "source": [
    "### Análise de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469a2032",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, metrics in cv_scores.items():\n",
    "    print(f\"\\n========= {model_name} =========\")\n",
    "    for metric_name, values in metrics.items():\n",
    "        print(f\"{metric_name:15}: Média {values.mean():.4f} | Std {values.std():.4f}\")\n",
    "\n",
    "# Plot Matriz de Confusão\n",
    "class_names = label_encoder.classes_\n",
    "\n",
    "for model_name, results in out_of_fold_preds.items():\n",
    "    cm = confusion_matrix(results['y_true'], results['y_pred'])\n",
    "    if hasattr(cm, 'get'): cm = cm.get() # Se for cupy, move para numpy\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'GTZAN - {model_name}')\n",
    "    plt.ylabel('Real')\n",
    "    plt.xlabel('Previsto')\n",
    "    plt.show()\n",
    "\n",
    "# MLP Treining Measurement\n",
    "if 'history' in locals():\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Gráfico de Acurácia\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'bo-', label='Acurácia no Treino')     # Linha Azul\n",
    "    plt.plot(epochs, val_acc, 'ro-', label='Acurácia na Validação') # Linha Vermelha\n",
    "    plt.title('Acurácia: Treino vs Validação (MLP)')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Acurácia')\n",
    "    plt.legend()\n",
    "\n",
    "    # Gráfico de Perda (Loss)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'bo-', label='Perda no Treino')\n",
    "    plt.plot(epochs, val_loss, 'ro-', label='Perda na Validação')\n",
    "    plt.title('Perda: Treino vs Validação (MLP)')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"A variável 'history' não foi encontrada. Certifique-se de ter rodado o treino do MLP.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-fma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
